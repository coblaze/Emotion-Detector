{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## How do words feel? Exploring Sentiment analysis and Emotion Detection"
      ],
      "metadata": {
        "id": "ECVaJPpFv5tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In this poroject, we'll delve into the fascinating world of sentiment analysis and emotion detection. We'll also tackle essential tasks like text preprocessing and feature engineering. Along the way, we'll explore a variety of machine learning techniques to create models that can classify and evaluate text data. To assess our models, we'll use a tool called a confusion matrix. Let's take a look at how this works by reviewing a few terminologies.\n",
        "\n",
        "##### Sentiment analysis and emotion detection are two essential techniques in natural language processing (NLP). Sentiment analysis assesses the overall sentiment of a sentence, categorizing it as positive, negative, or neutral offering insights to users reactions to products or brands. However, there are some limitations to this such as it's inability to capture the full spectrum of emotions. This is where emotion detection comes to play.\n",
        "\n",
        "##### Emotion detection identifes the specific emotions like sadness, anger, and happiness in text data. This is great because it offers buisnesses a more comprhensive understanding which makes facilitating informed decision making easier.\n"
      ],
      "metadata": {
        "id": "z-x-t5DymrB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Custom Classifier"
      ],
      "metadata": {
        "id": "b5PL3pnNvW8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### While there are many libraries available for prdicting sentiments in text, the same doesn't hold true for detecting emotions which is a bit more complex. In order to handle this problem we are going to take matters in our own hands and create a custom classifier. This classifier will claffiy emotions alsongside the sentiment prediction lirbaries to assess both the emotional and sentiment aspects of text.\n",
        "\n",
        "#####"
      ],
      "metadata": {
        "id": "bViVkpKrv3l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About the Dataset\n",
        "\n",
        "##### The GoEmotions dataset comprises 58,000 meticulously selected Reddit comments meticulously annotated across 27 distinct emotion categories alongside a neutral classification. These categories span a comprehensive spectrum of human emotional responses, encompassing complex nuances such as admiration, amusement, anger, and more. Each comment serves as a valuable data point, contributing to a profound understanding of how individuals express a diverse range of emotions within online communities. This dataset stands as a robust resource for academic and professional endeavors, offering rich insights into the intricate tapestry of human emotional experiences in digital communication. For access to the dataset, please follow this link: https://github.com/google-research/google-research/blob/master/goemotions/README.md"
      ],
      "metadata": {
        "id": "U-ZzOgyi4XmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_1.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_2.csv\n",
        "!wget -P data/full_dataset/ https://storage.googleapis.com/gresearch/goemotions/data/full_dataset/goemotions_3.csv"
      ],
      "metadata": {
        "id": "KC9lU-R4u0rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "f1 = pd.read_csv('/content/data/full_dataset/goemotions_1.csv')\n",
        "f2 = pd.read_csv('/content/data/full_dataset/goemotions_2.csv')\n",
        "f3 = pd.read_csv('/content/data/full_dataset/goemotions_3.csv')\n",
        "\n",
        "#data = pd.concat([f1, f2, f3], ignore_index= True)\n",
        "data = pd.concat([f1], ignore_index= True)\n",
        "#data.columns = ['text', '']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "7TSCCIT0qXKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is the DataFrame from the previous code\n",
        "# Melt the DataFrame to combine the emotion columns into a single 'emotion' column\n",
        "melted = data.melt(id_vars=['text', 'example_very_unclear'], value_vars=data.columns[10:], var_name='emotion', value_name='emotion_value')\n",
        "\n",
        "# Filtering to get rows where emotion_value is 1\n",
        "melted = melted[melted['emotion_value'] == 1]\n",
        "\n",
        "# Selecting only relevant columns\n",
        "result = melted[['text', 'emotion']]\n",
        "\n",
        "# Displaying the resulting DataFrame\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "KuUE4qNHaGML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.columns = ['text', 'emotion']\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "MoEV7FqsaMuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary mapping emotion column names to emotion names\n",
        "emotion_mapping = {\n",
        "    'admiration': 'admiration',\n",
        "    'amusement': 'amusement',\n",
        "    'anger': 'anger',\n",
        "    'annoyance': 'annoyance',\n",
        "    'approval': 'approval',\n",
        "    'caring': 'caring',\n",
        "    'confusion': 'confusion',\n",
        "    'curiosity': 'curiosity',\n",
        "    'desire': 'desire',\n",
        "    'disappointment': 'disappointment',\n",
        "    'disapproval': 'disapproval'\n",
        "}\n",
        "\n",
        "# Mapping the binary values to emotion names\n",
        "result['emotion'] = result['emotion'].map(emotion_mapping)\n",
        "\n",
        "\n",
        "# Reassigning result to our main dataframe\n",
        "data = result\n",
        "\n",
        "# Displaying the DataFrame with emotion names\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "agtT4NBPaPbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "WxJCxrleanoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning and Preprocessing\n",
        "\n",
        "##### At this point we've doone a slight bit of processing of the data to make it clear and readable. Instead of using the binary values to retrieve the emotions were going to use the labels for the emotions. Regauding data cleaning, it's important to perform to obtain better features and and accuracy. Some steps to do text preprocessing can be changing case, correcting spelling, removing special characters, punctuation, stop words, and normalization.\n",
        "\n",
        "##### In order to do this we're going to use the following libraries to preprocess the text."
      ],
      "metadata": {
        "id": "z6c8oIycfdUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import sklearn.feature_extraction.text as text\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "import xgboost\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, numpy, textblob, string\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "lupfG6TPouAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas nltk textblob"
      ],
      "metadata": {
        "id": "QdorhGClpOIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "G8zqGutupT7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "0o9e3pFdoxdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting uppercase to lowercase\n",
        "\n",
        "data['text'] = data['text'].apply(lambda a: \" \".join(a.lower() for a in a.split()))"
      ],
      "metadata": {
        "id": "NtXO3_fKpt1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing whitespace and special characters\n",
        "\n",
        "data['text'] = data['text'].apply(lambda a: \" \".join(a.replace('[^\\w\\s]','') for a in a.split()))"
      ],
      "metadata": {
        "id": "Cd9d9nHVp2Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What are stopwords and why remove them?***\n",
        "\n",
        "*Stopwords are common words like \"the,\" \"and,\" \"of,\" etc., which occur frequently in language but often carry little specific meaning in a sentence. Removing them from text analysis helps focus on more important, content-bearing words, streamlining the process by filtering out ubiquitous but less informative terms. This aids in better identifying the core, meaningful words for tasks like sentiment analysis, text classification, or information retrieval, enhancing the accuracy and relevance of the analysis.*"
      ],
      "metadata": {
        "id": "lPK4-BqRqZDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "data['text'] = data['text'].apply(lambda a: \" \".join(a for a in a.split() if a not in stop))"
      ],
      "metadata": {
        "id": "nq74mMtUqLix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "id": "kE3iSFzGa_PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    words = text.split()\n",
        "    misspelled_words = spell.unknown(words)\n",
        "    word_correction_mapping = {word: spell.correction(word) if spell.correction(word) is not None else word for word in misspelled_words}\n",
        "\n",
        "    for word in words:\n",
        "        if word in word_correction_mapping:\n",
        "            corrected_text.append(word_correction_mapping[word])\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "data['text'] = data['text'].apply(lambda a: correct_spellings(a))"
      ],
      "metadata": {
        "id": "HgyJYZYucmkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correcting mispelled words\n",
        "\n",
        "#data['text'] = data['text'].apply(lambda a: str(TextBlob(a).correct()))"
      ],
      "metadata": {
        "id": "n5LqzwZPrXMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is Stemming?***\n",
        "\n",
        "*Stemming is basically reducing words to their base or root form by removing prefixes and suffixes. This is important because we need to normalize the words  to their core meaning, so that similar variations are treated as a single word. This will help us simplify text analysis and imporve task like search and language processing.*\n",
        "\n",
        "*For instance, stemming converts words like **\"running\"**, **\"runs\"**, **\"ran\"**, to their common root **\"run\"**.*\n"
      ],
      "metadata": {
        "id": "5wZZ11oPr-9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing\n",
        "\n",
        "stem = PorterStemmer()\n",
        "data['text'] = data['text'].apply(lambda a: \" \".join([stem.stem(word) for word in a.split()]))"
      ],
      "metadata": {
        "id": "qBmjxR5_rrc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Numeric Transformation of Categorical Data***\n",
        "\n",
        "*Converting categorical values to numerical values is valuable for this analysis beacuse many machine learning algorithms and statistical models work better with numerical inputs. Using  python's label encoder function helps translate categories into numeric representations, enabling the algorithms to effectivly interpret the data. In our analysis we will use the function to label the emotions.*"
      ],
      "metadata": {
        "id": "BuaVoiRMw-JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['emotion'].value_counts()"
      ],
      "metadata": {
        "id": "vMGMRSpLy7pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming emotion categories to numerical categories\n",
        "\n",
        "labelE = preprocessing.LabelEncoder()\n",
        "data['emotion'] = labelE.fit_transform(data['emotion'])"
      ],
      "metadata": {
        "id": "MpBQVgQmwzEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['emotion'].value_counts()"
      ],
      "metadata": {
        "id": "STUfN7hf2gEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking data after preprocessing\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "s9SWCVeovqPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Split"
      ],
      "metadata": {
        "id": "pokqO0lKwIRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = model_selection.train_test_split(data['text'], data['emotion'],stratify= data['emotion'])"
      ],
      "metadata": {
        "id": "KixiZTj42-w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is feature engineering?***\n",
        "\n",
        "*Feature engineering involves shaping and refining data to improve predictive models. Our focus here is to create or modify features that better capture the essence of the data, especially in textual content. Leveraging methods like count vectorization and TF-IDF (Term Frequency-Inverse Document Frequency), we convert text into numerical representations, highlighting important patterns within the data. Count vectorization quantifies the occurrence of words in text, while TF-IDF reflects the significance of words in a document compared to their occurrence in a broader collection of documents. These techniques are instrumental in converting unstructured text into structured, numeric form, empowering machine learning models to extract meaningful insights from the text.*"
      ],
      "metadata": {
        "id": "PzqYHg1240Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the CountVectorizer\n",
        "countV = CountVectorizer()\n",
        "\n",
        "# Fit the CountVectorizer with the 'text' data from the entire dataset\n",
        "countV.fit(data['text'])\n",
        "\n",
        "# Transform the text data of the training set (Xtrain) into a document-term matrix\n",
        "cv_xtrain = countV.transform(Xtrain)\n",
        "\n",
        "# Transform the text data of the testing set (Xtest) into a document-term matrix using the same CountVectorizer\n",
        "cv_xtest = countV.transform(Xtest)\n"
      ],
      "metadata": {
        "id": "zT1_FGDz63ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF Vectorizer instance\n",
        "tVect = TfidfVectorizer()\n",
        "\n",
        "# Fit the TF-IDF Vectorizer with the 'text' data from the entire dataset\n",
        "tVect.fit(data['text'])\n",
        "\n",
        "# Transform the text data of the training set (Xtrain) using the TF-IDF Vectorizer\n",
        "tv_xtrain = tVect.transform(Xtrain)\n",
        "\n",
        "# Transform the text data of the testing set (Xtest) using the same TF-IDF Vectorizer\n",
        "tv_xtest = tVect.transform(Xtest)\n"
      ],
      "metadata": {
        "id": "SFCCNg7J8NZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build(model, X_train, target, X_test):\n",
        "  # Train the model\n",
        "  model.fit(X_train, target)\n",
        "\n",
        "  # Predict using the trained model\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  # Calculate and return accuracy\n",
        "  return metrics.accuracy_score(predictions, Ytest)"
      ],
      "metadata": {
        "id": "vRRqudTT99H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is the Multinominal Naive Bayes?***\n",
        "\n",
        "Can you expand on what the multinominal naive bayes algorithm?\n",
        "\n",
        "The multinomial naive Bayes algorithm essentially calculates the probability of each category using the Bayes theorem."
      ],
      "metadata": {
        "id": "bZp6cShfCuk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Model with count vectors\n",
        "\n",
        "cv_NBresult = build(naive_bayes.MultinomialNB(), cv_xtrain, Ytrain, cv_xtest)\n",
        "\n",
        "print(cv_NBresult)"
      ],
      "metadata": {
        "id": "TUgIX-1t9W1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Model with count vectors\n",
        "\n",
        "tv_NBresult = build(naive_bayes.MultinomialNB(), tv_xtrain, Ytrain, tv_xtest)\n",
        "\n",
        "print(tv_NBresult)"
      ],
      "metadata": {
        "id": "3Yf5UGPSFWlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***What is Random Forest?***\n",
        "\n",
        "*Can you expand on what random forest is?*\n",
        "\n",
        "The random forest essentially calculates the probability of each category using the Bayes theorem.*"
      ],
      "metadata": {
        "id": "HYE6ynHEH3dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_RFresult = build(ensemble.RandomForestClassifier(), cv_xtrain, Ytrain, cv_xtest)\n",
        "\n",
        "print(cv_RFresult)"
      ],
      "metadata": {
        "id": "BrCpWsMYJ6zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tv_RFresult = build(ensemble.RandomForestClassifier(), tv_xtrain, Ytrain, tv_xtest)\n",
        "\n",
        "print(tv_RFresult)"
      ],
      "metadata": {
        "id": "prccH32PKVGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix"
      ],
      "metadata": {
        "id": "616m-zQZL9ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = linear_model.LogisticRegression().fit(tv_xtrain, Ytrain)\n",
        "val_predictions = classifier.predict(tv_xtest)\n",
        "\n",
        "# Precision , Recall , F1 - score , Support\n",
        "y_true, y_pred = Ytest, val_predictions\n",
        "print(classification_report(y_true, y_pred))\n",
        "print()"
      ],
      "metadata": {
        "id": "WP8wRsTwMCM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Twitter API"
      ],
      "metadata": {
        "id": "Goq_I4Ca3fCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xOjqK3jZ83pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tData = []"
      ],
      "metadata": {
        "id": "2qycZ46ZqT_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    'api_key':'ENTER YOUR SCRAPER API KEY',\n",
        "    'query':'Meta',\n",
        "    'num':'500'\n",
        "\n",
        "}\n",
        "\n",
        "res = requests.get(\n",
        "    'https://api.scraperapi.com/structured/twitter/search',params = payload\n",
        ")\n",
        "\n",
        "data = res.json()"
      ],
      "metadata": {
        "id": "cMLJ465hqXz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "id": "gERxuPUHNh5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allTweets = data['organic_results']\n",
        "for tweet in allTweets:\n",
        "  tData.append(tweet)"
      ],
      "metadata": {
        "id": "IhuWMRWMlJUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(tData)\n",
        "df.to_json('tweets.json', orient = 'index')\n",
        "print(\"exported\")"
      ],
      "metadata": {
        "id": "SFJfT5FDL1us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YyC0JzhdOpm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt = pd.read_json('tweets.json', lines = True, orient = 'records')"
      ],
      "metadata": {
        "id": "150qKnTYO7I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt = twt.to_csv('twt.csv', index = False)"
      ],
      "metadata": {
        "id": "Vr5C9ue_Pbdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt = pd.read_csv('twt.csv')"
      ],
      "metadata": {
        "id": "oNuLIeAASrkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt = df[['snippet']]"
      ],
      "metadata": {
        "id": "OfMol3ExTUbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt.tail()"
      ],
      "metadata": {
        "id": "L2MOwuYbTfCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xpredict = twt['snippet']\n",
        "\n",
        "pred_tfidf = tVect.transform(Xpredict)\n",
        "twt['Emotion'] = classifier.predict(pred_tfidf)\n",
        "twt.tail() #Change twt"
      ],
      "metadata": {
        "id": "l0YerRSIUctX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twt['sentiment'] = twt['snippet'].apply(lambda a: TextBlob(a).sentiment[0] )\n",
        "def function (value):\n",
        "     if value['sentiment'] < 0 :\n",
        "        return 'Negative'\n",
        "     if value['sentiment'] > 0 :\n",
        "        return 'Positive'\n",
        "     return 'Neutral'\n",
        "\n",
        "twt['Sentiment_label'] = twt.apply (lambda a: function(a),axis=1)\n",
        "twt.tail()"
      ],
      "metadata": {
        "id": "R9isXhsCVe67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install chart_studio"
      ],
      "metadata": {
        "id": "32gTAj4m23mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chart_studio.plotly as py\n",
        "import plotly as ply\n",
        "import cufflinks as cf\n",
        "from plotly.graph_objs import *\n",
        "from plotly.offline import *\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
        "\n",
        "Sentiment_df = pd.DataFrame(twt.Sentiment_label.value_counts().reset_index())\n",
        "Sentiment_df.columns = ['sentiment', 'Count']\n",
        "Sentiment_df = pd.DataFrame(Sentiment_df)\n",
        "Sentiment_df['Percentage'] = 100 * Sentiment_df['Count']/ Sentiment_df['Count'].sum()\n",
        "Sentiment_Max = Sentiment_df.iloc[0,0]\n",
        "\n",
        "\n",
        "Sentiment_percent = str(round(Sentiment_df.iloc[0,2],2))\n",
        "fig1 = Sentiment_df.iplot(kind='pie',labels='sentiment',values='Count',textinfo='label+percent', title= 'Sentiment Analysis', world_readable=True,\n",
        "                    asFigure=True)\n",
        "ply.offline.plot(fig1,filename=\"sentiment\")\n",
        "\n",
        "# Use IPython's display() function to read and display the HTML file\n",
        "display(HTML(filename='Sentiment.html'))"
      ],
      "metadata": {
        "id": "lM2kDH6dXGNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chart_studio.plotly as py\n",
        "import plotly as ply\n",
        "import cufflinks as cf\n",
        "from plotly.graph_objs import *\n",
        "from plotly.offline import *\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
        "Emotion_df = pd.DataFrame(twt.Emotion.value_counts().reset_index())\n",
        "Emotion_df.columns = ['Emotion', 'Count']\n",
        "Emotion_df = pd.DataFrame (Emotion_df)\n",
        "\n",
        "# Convert 'Emotion' column to string type\n",
        "#Emotion_df['Emotion'] = Emotion_df['Emotion'].astype(str)\n",
        "\n",
        "Emotion_df['Percentage'] = 100 * Emotion_df['Count']/ Emotion_df['Count'].sum()\n",
        "Emotion_Max = Emotion_df.iloc[0,0]\n",
        "Emotion_percent = str(round(Emotion_df.iloc[0,2],2))\n",
        "fig = Emotion_df.iplot(kind='pie', labels = 'Emotion', values = 'Count',pull= .2, hole=.2 , colorscale = 'reds', textposition='outside',colors=['red','green','purple','orange','blue','yellow','pink'],textinfo='label+percent', title= 'Emotion Analysis', world_readable=True,asFigure=True)\n",
        "ply.offline.plot(fig,filename=\"Emotion\")\n",
        "\n",
        "\n",
        "# Use IPython's display() function to read and display the HTML file\n",
        "display(HTML(filename='Emotion.html'))"
      ],
      "metadata": {
        "id": "VdAwcuB6XHhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0X0eiUPmOIJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZaQd8jXKH0At"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSzr6ZID8TX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P5xG688R8IEa"
      }
    }
  ]
}